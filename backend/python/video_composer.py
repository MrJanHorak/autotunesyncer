import os
import re 
import gc
import threading
import subprocess
import sys
import logging
import traceback
import os.path
import math
import shutil
import tempfile
import cProfile
import pstats
import threading
import asyncio
import time
import numpy as np
import torch
from pathlib import Path
from threading import RLock

# Optional audio analysis
try:
    import aubio
    AUBIO_AVAILABLE = True
except ImportError:
    aubio = None
    AUBIO_AVAILABLE = False
    logging.warning("aubio not available, some audio analysis features disabled")

import syncio
from pstats import SortKey

# Optional tqdm for progress bars
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    # Fallback tqdm implementation
    class tqdm:
        def __init__(self, iterable=None, total=None, **kwargs):
            self.iterable = iterable
            self.total = total
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
        def __iter__(self):
            return iter(self.iterable) if self.iterable else iter([])
        def update(self, n=1):
            pass
    TQDM_AVAILABLE = False
    logging.warning("tqdm not available, progress bars disabled")

import time
from contextlib import contextmanager
from cachetools import LRUCache

# Third-party imports
import numpy as np
import torch

from moviepy import (
    VideoFileClip,
    clips_array,
    CompositeVideoClip,
    ColorClip,
    concatenate_videoclips
)

from concurrent.futures import ThreadPoolExecutor, as_completed

import weakref

from contextlib import contextmanager

try:
    from utils import normalize_instrument_name, midi_to_note
except ImportError:
    # Fallback implementation
    def normalize_instrument_name(name):
        """Match frontend's normalizeInstrumentName"""
        return name.lower().replace(' ', '_')
    
    def midi_to_note(midi_num):
        """Convert MIDI note number to note name"""
        notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        note_name = notes[midi_num % 12]
        octave = (midi_num // 12) - 1
        return f"{note_name}{octave}"
from drum_utils import (
    DRUM_NOTES,
    # process_drum_track,
    # get_drum_groups,
    # get_drum_name,
    is_drum_kit
)

from processing_utils import encoder_queue, GPUManager
from get_system_metrics import get_system_metrics
from optimized_autotune_cache import OptimizedAutotuneCache
from video_utils import run_ffmpeg_command, encode_video, validate_video
from cuda_compositing import CudaVideoProcessor
import traceback

from gpu_pipeline import GPUPipelineProcessor
from path_registry import PathRegistry

import mmap
# from contextlib import ExitStack

from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('video_processing.log', mode='w', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ],
    force=True
)

# Configure stream handler to handle Unicode properly on Windows
for handler in logging.getLogger().handlers:
    if isinstance(handler, logging.StreamHandler):
        # Ensure proper encoding for Windows console
        if hasattr(handler.stream, 'reconfigure'):
            try:
                handler.stream.reconfigure(encoding='utf-8', errors='replace')
            except:
                pass

@contextmanager
def timing_block(name):
    """Context manager to measure execution time"""
    start = time.perf_counter()
    try:
        yield
    finally:
        duration = time.perf_counter() - start
        logging.info(f"Operation [{name}] took {duration:.3f} seconds")

class GPUStreamManager:
    def __init__(self, num_streams=4):
        self.streams = [torch.cuda.Stream() for _ in range(num_streams)]
        self.stream_idx = 0
        self.lock = threading.Lock()

    def get_stream(self):
        with self.lock:
            stream = self.streams[self.stream_idx]
            self.stream_idx = (self.stream_idx + 1) % len(self.streams)
            return stream

    def synchronize_all(self):
        for stream in self.streams:
            torch.cuda.synchronize(stream.device)

class ClipPool:
    def __init__(self, max_size=8):
        self.semaphore = threading.BoundedSemaphore(max_size)
        self.clips = weakref.WeakSet()
    
    @contextmanager
    def acquire(self):
        self.semaphore.acquire()
        try:
            clip = None
            yield clip
        finally:
            if clip:
                clip.close()
            self.semaphore.release()


class MMAPHandler:
    def __init__(self):
        self.mapped_files = {}
        
    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
        
    def map_file(self, file_path):
        if file_path not in self.mapped_files:
            with open(file_path, 'rb') as f:
                self.mapped_files[file_path] = mmap.mmap(
                    f.fileno(), 0, access=mmap.ACCESS_READ)
        return self.mapped_files[file_path]
        
    def cleanup(self):
        for mmap_obj in self.mapped_files.values():
            try:
                mmap_obj.close()
            except:
                pass
        self.mapped_files.clear()


class ClipManager:
    def __init__(self):
        self.active_clips = weakref.WeakSet()
        
    @contextmanager
    def managed_clip(self, clip):
        try:
            self.active_clips.add(clip)
            yield clip
        finally:
            try:
                clip.close()
            except:
                pass
            self.active_clips.discard(clip)

class ProgressTracker:
    def __init__(self, total_notes):
        self.progress_bar = tqdm(total=total_notes, 
                               desc="Processing notes",
                               unit="note")
        self.completed = 0
        self.failed = 0
        
    def update(self, success=True):
        self.completed += 1
        if not success:
            self.failed += 1
        self.progress_bar.update(1)
        self.progress_bar.set_postfix({
            "success_rate": f"{(self.completed-self.failed)/self.completed*100:.1f}%"
        })

    def close(self):
        self.progress_bar.close()

class VideoComposerConfig:
    def __init__(self):
        self.CHUNK_DURATION = 4
        self.OVERLAP_DURATION = 1
        self.CROSSFADE_DURATION = 0.5
        self.MIN_VIDEO_DURATION = 1.0
        self.DURATION = 1.0
        self.VOLUME_MULTIPLIERS = {
            'drums': 0.2,
            'instruments': 1.5
        }

class VideoComposer:

    FRAME_RATE = 30
    CHUNK_DURATION = 4
    OVERLAP_DURATION = 1
    CROSSFADE_DURATION = 0.5
    MIN_VIDEO_DURATION = 1.0
    DURATION = 1.0
    VOLUME_MULTIPLIERS = {
        'drums': 0.2,        'instruments': 1.5
    }

    def __init__(self, processed_videos_dir, midi_data, output_path):
        """Initialize VideoComposer with proper path handling"""
        try:
            logging.info("=== VideoComposer Initialization ===")
            logging.info(f"Received MIDI data structure: {list(midi_data.keys())}")
            logging.info(f"Grid arrangement from MIDI: {midi_data.get('gridArrangement')}")

            self.processed_videos_dir = Path(processed_videos_dir)
            
            # Check if we have an uploads directory provided or need to find it
            if 'uploadsDir' in midi_data:
                self.uploads_dir = Path(midi_data['uploadsDir'])
                logging.info(f"Using provided uploads directory: {self.uploads_dir}")
            else:
                # Try to find the uploads directory - look for backend/uploads
                current_dir = Path(processed_videos_dir)
                backend_uploads = None
                
                # Try multiple possible paths
                possible_paths = [
                    Path(__file__).parent.parent / "uploads",  # From backend/python -> backend/uploads
                    current_dir.parent.parent.parent / "uploads",  # From temp dir back to uploads
                    current_dir / "uploads",  # Direct uploads subdirectory
                ]
                
                for path in possible_paths:
                    if path.exists() and any(f.name.startswith("processed_") and f.name.endswith(".mp4") for f in path.iterdir()):
                        backend_uploads = path
                        break
                
                if backend_uploads:
                    self.uploads_dir = backend_uploads
                    logging.info(f"Found uploads directory: {self.uploads_dir}")
                else:
                    # Fall back to creating uploads in processed_videos_dir
                    self.uploads_dir = self.processed_videos_dir / "uploads"
                    logging.info(f"Creating uploads directory: {self.uploads_dir}")
                    self.uploads_dir.mkdir(exist_ok=True)
                    
            # Verify uploads directory exists
            if not self.uploads_dir.exists():
                raise ValueError(f"Uploads directory not found: {self.uploads_dir}")
                
            self.config = VideoComposerConfig()
            self.gpu_manager = GPUManager()
            self.output_path = output_path
            self.midi_data = midi_data
            self._setup_paths(processed_videos_dir, output_path)
            self.midi_data = midi_data
            self._process_midi_data(midi_data)
            self._setup_track_configuration()
            self.clip_manager = ClipManager()
            self.gpu_stream_manager = GPUStreamManager()
            self.chunk_cache = {}
            self.chunk_cache_locks = {i: threading.Lock() for i in range(1000)} # Pre-allocate locks
            self.chunk_cache = LRUCache(128)  # Example: Cache up to 128 chunks
            # self.chunk_cache = {}  # This would overwrite the LRUCache
            self.max_cached_chunks = 4
            self.metrics_log = []
            self.encoder_params = {
                'codec': 'h264_nvenc',
                'preset': 'p4',
                'ffmpeg_params': [
                    "-vsync", "cfr",
                    "-c:v", "h264_nvenc",
                    "-preset", "p4",
                    "-surfaces", "16",
                    "-tune", "hq",
                    "-rc", "vbr_hq",
                    "-cq", "20",
                    "-b:v", "5M",
                    "-maxrate", "10M",
                    "-bufsize", "10M",
                    "-profile:v", "high"
                ]
            }
            self.chunk_size = max(1, min(16, os.cpu_count()))  # Smaller chunk size
            self.max_workers = min(2, os.cpu_count())  # Limit workers            self.use_gpu = True
            self.lock = RLock()  # Add class-level lock
            self.clip_pool = ClipPool(max_size=8)  # Add clip pool
            self.chunk_cache_lock = RLock()  # Add dedicated cache lock
            self.max_cache_size = 1024 * 1024 * 100
            self.active_readers = set()  # Add reader tracking
            # Initialize path registry - use singleton instance
            self.path_registry = PathRegistry.get_instance()
            
            # After copying files, register them
            self._register_video_paths()
            self.video_cache = LRUCache(maxsize=64)  # Increase cache size
            self.audio_cache = LRUCache(maxsize=64) 
            self.autotune_cache = LRUCache(maxsize=64) # Keep for backwards compatibility
            # Initialize optimized autotune cache system
            self.optimized_cache = OptimizedAutotuneCache(max_workers=self.max_workers)
            self._tuned_videos_cache = {}  # Cache for preprocessed tuned videos
            # Log track information
            logging.info(f"Regular tracks: {len(self.tracks)}")
            logging.info(f"Drum tracks: {len(self.drum_tracks)}")
            for track in self.drum_tracks:
                logging.info(f"Drum track found: {track.get('instrument', {}).get('name')}")
            try:
                import torch
                self.has_cuda = torch.cuda.is_available()
                if self.has_cuda:
                    # Initialize CUDA context
                    device = torch.device('cuda:0')
                    torch.cuda.set_device(device)
                    # Create empty tensor to initialize CUDA
                    _ = torch.zeros(1, device=device)
                    logging.info(f"CUDA initialized successfully: {torch.cuda.get_device_name(0)}")
                    
                    # Set optimal settings for video processing
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cuda.matmul.allow_tf32 = True
                    torch.backends.cudnn.allow_tf32 = True
                else:
                    logging.warning("CUDA not available, using CPU processing")
            except Exception as e:
                self.has_cuda = False
                logging.error(f"Error initializing CUDA: {e}")
                            
        except Exception as e:
            logging.error(f"VideoComposer init error: {str(e)}")
            raise

    @staticmethod
    def normalize_midi_timing(midi_data):
        """Adjust all note timings to start at the same point"""
        # Find earliest note time across all tracks
        min_time = float('inf')
        for track in midi_data['tracks']:
            if track.get('notes'):
                track_min = min(float(note.get('time', 0)) for note in track['notes'])
                min_time = min(min_time, track_min)
          # Shift all notes to start at 0
        for track in midi_data['tracks']:
            for note in track.get('notes', []):
                note['time'] = float(note['time']) - min_time
        
        return midi_data
    
    def _register_video_paths(self):
        """Register paths for all videos in uploads directory"""
        # Get singleton instance
        registry = PathRegistry.get_instance()
        
        # Set registry file if needed
        registry_file = self.processed_videos_dir / "path_registry.json"
        
        # Register from uploads directory where videos are actually stored
        logging.info(f"Scanning uploads directory for videos: {self.uploads_dir}")
        
        # Use the new specialized method for uploads directory
        success = registry.register_from_uploads_directory(self.uploads_dir)
        
        if not success:
            logging.warning("No videos found in uploads directory, trying processed videos directory as fallback")
            # Also register from processed videos directory (fallback)
            registry.register_from_directory(self.processed_videos_dir)
        
        # Save registry for debugging
        registry.save_registry(str(registry_file))
        
        # Debug dump to help troubleshoot
        registry.debug_dump()
        
        # Log registration stats
        stats = registry.get_stats()
        logging.info(f"Path registry stats: {stats}")
        logging.info(f"Total videos registered: {stats['total_paths']}")

    def encode_video(self, cmd):
        logging.info(f"Encoding video with command: {' '.join(cmd)}")
        result = encode_video(cmd)
        return result

    def validate_video(self, output_path):
        validate_video(output_path)


    def _log_metrics(self):
        """Log system metrics during processing"""
        metrics = get_system_metrics()
        self.metrics_log.append(metrics)
        
        gpu_info = f", GPU={metrics.get('gpu_util', 'N/A')}%, GPU Memory={metrics.get('gpu_memory', 'N/A')}%" if metrics.get('gpu_util') is not None else ""
        
        logging.info(
            f"System metrics: CPU={metrics['cpu_percent']}%, "
            f"Memory={metrics['memory_percent']}%"
            f"{gpu_info}"
        )
  
    def _setup_paths(self, processed_videos_dir, output_path):
        """Setup and validate paths"""
        dir_path = (processed_videos_dir['processed_videos_dir'] 
                   if isinstance(processed_videos_dir, dict) 
                   else str(processed_videos_dir))
        
        self.processed_videos_dir = Path(dir_path).resolve()
        self.output_path = Path(output_path)
        self.temp_dir = self.processed_videos_dir
        
        if not self.processed_videos_dir.exists():
            raise ValueError(f"Directory not found: {self.processed_videos_dir}")
            
        self.temp_dir.mkdir(exist_ok=True, parents=True)
        logging.info(f"Resolved path: {self.processed_videos_dir}")

    def debug_midi_track_timing(self, midi_data):
        """Debug function to print detailed note timing for all tracks"""
        logging.info("\n=== DETAILED MIDI NOTE TIMING ===")
        
        for track_idx, track in enumerate(midi_data['tracks']):
            instrument = track.get('instrument', {}).get('name', f'Unknown-{track_idx}')
            notes = track.get('notes', [])
            
            if not notes:
                logging.info(f"Track {track_idx} ({instrument}): NO NOTES")
                continue
                
            # Sort notes by time
            sorted_notes = sorted(notes, key=lambda n: float(n.get('time', 0)))
            first_time = float(sorted_notes[0].get('time', 0))
            last_time = float(sorted_notes[-1].get('time', 0))
            
            logging.info(f"Track {track_idx} ({instrument}): {len(notes)} notes")
            logging.info(f"  Time range: {first_time:.2f}s to {last_time:.2f}s")
            # FIX: Use double quotes for inner dictionary keys
            note_times = [f"{float(n.get('time', 0)):.2f}s" for n in sorted_notes[:5]]
            logging.info(f"  First 5 notes: {', '.join(note_times)}")

    def _analyze_midi_timing(self):
        """Analyze and log detailed timing information for all tracks"""
        logging.info("\n=== DETAILED MIDI TIMING ANALYSIS ===")
        
        # Analyze each track
        for track_id, track in self.tracks.items():
            instrument_name = track.get('instrument', {}).get('name', 'unknown')
            notes = track.get('notes', [])
            if not notes:
                continue
                
            # Sort notes by time
            sorted_notes = sorted(notes, key=lambda n: float(n.get('time', 0)))
            
            # Get time range statistics
            first_note_time = float(sorted_notes[0].get('time', 0))
            last_note_time = float(sorted_notes[-1].get('time', 0)) + float(sorted_notes[-1].get('duration', 0))
            total_duration = last_note_time - first_note_time
            note_count = len(notes)
            
            # Calculate which chunks these notes belong to
            first_chunk = int(first_note_time / self.CHUNK_DURATION)
            last_chunk = int(last_note_time / self.CHUNK_DURATION)
            chunk_count = last_chunk - first_chunk + 1
            
            # Group notes by chunk
            notes_by_chunk = {}
            for chunk_idx in range(first_chunk, last_chunk + 1):
                chunk_start = chunk_idx * self.CHUNK_DURATION
                chunk_end = chunk_start + self.CHUNK_DURATION
                
                # Count notes in this chunk time range
                chunk_notes = [
                    note for note in sorted_notes
                    if chunk_start <= float(note.get('time', 0)) < chunk_end
                ]
                notes_by_chunk[chunk_idx] = len(chunk_notes)
            
            # Log detailed timing info for this track
            logging.info(f"\nTrack {track_id}: {instrument_name}")
            logging.info(f"  Total notes: {note_count}")
            logging.info(f"  Time range: {first_note_time:.2f}s to {last_note_time:.2f}s (duration: {total_duration:.2f}s)")
            logging.info(f"  Chunks: {first_chunk} to {last_chunk} (spans {chunk_count} chunks)")
            
            # Log distribution of notes per chunk
            chunk_log = "  Notes per chunk: "
            for chunk_idx, count in sorted(notes_by_chunk.items()):
                chunk_log += f"[{chunk_idx}:{count}] "
            logging.info(chunk_log)
            
            # Log first 5 note times for verification
            note_times = [float(note.get('time', 0)) for note in sorted_notes[:5]]
            logging.info(f"  First 5 note times: {', '.join([f'{t:.2f}s' for t in note_times])}")
            
            # # For piano track specifically, log more details
            # if 'piano' in instrument_name.lower():
            #     logging.info(f"  === PIANO TRACK DETAILED ANALYSIS ===")
            #     all_note_times = [float(note.get('time', 0)) for note in sorted_notes]
            #     # Find any unusual gaps in timing
            #     note_gaps = [all_note_times[i+1] - all_note_times[i] for i in range(len(all_note_times)-1)]
            #     avg_gap = sum(note_gaps) / max(1, len(note_gaps))
            #     max_gap = max(note_gaps) if note_gaps else 0
                
            #     logging.info(f"  Piano avg note gap: {avg_gap:.2f}s, max gap: {max_gap:.2f}s")
            #     # Log all piano note times for detailed investigation
            #     logging.info(f"  Piano note timestamps (first 20): {', '.join([f'{t:.2f}s' for t in all_note_times[:20]])}")

    def _process_midi_data(self, midi_data):
        """Process MIDI data with correct handling of duplicate instruments"""
        if not isinstance(midi_data, dict):
            raise ValueError(f"Expected dict for midi_data, got {type(midi_data)}")
            
        if 'tracks' not in midi_data:
            raise ValueError("Missing 'tracks' in midi_data")
        
        self.debug_midi_track_timing(midi_data)

        # Add normalization here - before any track processing
        logging.info("\n=== Normalizing MIDI Timing ===")
        midi_data = VideoComposer.normalize_midi_timing(midi_data)
        logging.info("MIDI timing normalized - all tracks now start from the same time reference")

        self.debug_midi_track_timing(midi_data)

        tracks = midi_data['tracks']
        self.tracks = {}
        self.drum_tracks = []
        self.regular_tracks = []
        
        logging.info(f"\n=== Processing Tracks ===")
        logging.info(f"MIDI track structure: {type(tracks)}, {'dict keys' if isinstance(tracks, dict) else 'list length'}: {len(tracks)}")
        
        # List all preprocessed files for drum processing
        upload_files = list(self.uploads_dir.glob('processed_*.mp4'))
        logging.info(f"\nPreprocessed files in ({self.uploads_dir}):")
        for file in upload_files:
            logging.info(f"Found preprocessed file: {file.name}")

        # Process tracks - ensure we preserve all tracks including duplicates
        for idx, track in enumerate(tracks if isinstance(tracks, list) else tracks.values()):
            # Always use the original track index as the track ID to prevent overwriting
            track_id = str(idx)
            
            # Store ID in track data itself for later lookup
            if isinstance(track, dict):
                track['id'] = track_id
                track['original_index'] = idx  # Store original index for reference
                
            normalized_track = self._normalize_track(track)
            
            # Check if it's a drum track
            is_drum = (
                normalized_track.get('isDrum') or 
                normalized_track.get('instrument', {}).get('isDrum') or
                normalized_track.get('channel') == 9
            )
            
            if is_drum:
                logging.info(f"\nProcessing drum track {idx}/{track_id}")
                drum_dir = self.processed_videos_dir / f"track_{idx}_drums"
                drum_dir.mkdir(exist_ok=True)
                
                # Get unique drum types needed from notes
                needed_drums = set()
                for note in normalized_track.get('notes', []):
                    midi_note = note.get('midi')
                    drum_name = DRUM_NOTES.get(midi_note)
                    if drum_name:
                        needed_drums.add((midi_note, drum_name))
                          # Process each needed drum type
                for midi_note, drum_name in needed_drums:
                    normalized_name = f"drum_{drum_name.lower().replace(' ', '_')}"
                    
                    # Find preprocessed drum file
                    for file in upload_files:
                        if normalized_name in file.name.lower():
                            dest_file = drum_dir / f"{normalized_name}.mp4"
                            # Copy preprocessed file to drum directory
                            try:
                                shutil.copy2(str(file), str(dest_file))
                                logging.info(f"Copied preprocessed drum file: {file.name} -> {dest_file}")
                            except Exception as e:
                                logging.error(f"Error copying drum file: {e}")
                            break
                        else:
                            logging.warning(f"No preprocessed file found for {normalized_name}")
                
                # Add drum track to drum_tracks (outside the loop)
                self.drum_tracks.append(normalized_track)
                logging.info(f"Added drum track {track_id}: {normalized_track.get('instrument', {}).get('name')}")
            else:
                # Handle regular (non-drum) tracks
                self.regular_tracks.append(normalized_track)
                # IMPORTANT: Use track_id (index) to store track, not instrument name
                self.tracks[track_id] = normalized_track
                logging.info(f"Added regular track {track_id}: {normalized_track.get('instrument', {}).get('name')}")

            logging.info(f"\nProcessed {len(self.regular_tracks)} regular tracks and {len(self.drum_tracks)} drum tracks")

    def _process_single_track(self, track_id, track):
        """Process a single track with consistent ID handling"""
        normalized_track = self._normalize_track(track)
        
        # Check if it's a drum track
        is_drum = (
            normalized_track.get('isDrum') or 
            normalized_track.get('instrument', {}).get('isDrum') or
            normalized_track.get('channel') == 9 or
            any(name in normalized_track.get('instrument', {}).get('name', '').lower() 
                for name in ['drum', 'percussion', 'kit'])
        )
        
        if is_drum:
            self.drum_tracks.append(normalized_track)
        else:
            self.regular_tracks.append(normalized_track)
            self.tracks[track_id] = normalized_track

    # def _process_midi_data(self, midi_data):
    #     """Process MIDI data and organize tracks"""
    #     if not isinstance(midi_data, dict):
    #         raise ValueError(f"Expected dict for midi_data, got {type(midi_data)}")
            
    #     if 'tracks' not in midi_data:
    #         raise ValueError("Missing 'tracks' in midi_data")

    #     tracks = midi_data['tracks']
    #     self.tracks = {}
    #     self.drum_tracks = []
    #     self.regular_tracks = []
        
    #     logging.info("\n=== Processing Tracks ===")
        
    #     # List all preprocessed files
    #     upload_files = list(self.uploads_dir.glob('processed_*.mp4'))
    #     logging.info(f"\nPreprocessed files in ({self.uploads_dir}):")
    #     for file in upload_files:
    #         logging.info(f"Found preprocessed file: {file.name}")

    #     # Check if tracks is a dict with actual IDs
    #     if isinstance(tracks, dict):
    #         track_items = tracks.items()
    #     else:
    #         # If it's a list, enumerate it
    #         track_items = enumerate(tracks)
            
    #     for idx_or_id, track in track_items:
    #         # Use the original ID if available (from dict keys), otherwise use index
    #         track_id = str(idx_or_id)
            
    #         # Important: Store the ID in the track data for later reference
    #         if isinstance(track, dict):
    #             track['id'] = track_id
                
    #         normalized_track = self._normalize_track(track)

    #     for idx, track in enumerate(tracks if isinstance(tracks, list) else tracks.values()):
    #         track_id = str(idx)
    #         normalized_track = self._normalize_track(track)
            
    #         # Check if it's a drum track
    #         is_drum = (
    #             normalized_track.get('isDrum') or 
    #             normalized_track.get('instrument', {}).get('isDrum') or
    #             normalized_track.get('channel') == 9 or
    #             any(name in normalized_track.get('instrument', {}).get('name', '').lower() 
    #                 for name in ['drum', 'percussion', 'kit'])
    #         )
            
    #         if is_drum:
    #             logging.info(f"\nProcessing drum track {track_id}")
    #             drum_dir = self.processed_videos_dir / f"track_{idx}_drums"
    #             drum_dir.mkdir(exist_ok=True)
                
    #             # Get unique drum types needed from notes
    #             needed_drums = set()
    #             for note in normalized_track.get('notes', []):
    #                 midi_note = note.get('midi')
    #                 drum_name = DRUM_NOTES.get(midi_note)
    #                 if drum_name:
    #                     needed_drums.add((midi_note, drum_name))
                        
    #             # Process each needed drum type
    #             for midi_note, drum_name in needed_drums:
    #                 normalized_name = f"drum_{drum_name.lower().replace(' ', '_')}"
                    
    #                 # Find preprocessed drum file
    #                 for file in upload_files:
    #                     if normalized_name in file.name.lower():
    #                         dest_file = drum_dir / f"{normalized_name}.mp4"
    #                         # Copy preprocessed file to drum directory
    #                         shutil.copy2(str(file), str(dest_file))
    #                         logging.info(f"Copied preprocessed drum file: {file.name} -> {dest_file}")
    #                         break
    #                 else:
    #                     logging.warning(f"No preprocessed file found for {normalized_name}")
                
    #             self.drum_tracks.append(normalized_track)
                
    #         else:
    #             self.regular_tracks.append(normalized_track)
    #             self.tracks[track_id] = normalized_track

    #     logging.info(f"\nProcessed {len(self.tracks)} regular tracks and {len(self.drum_tracks)} drum tracks")


    def _normalize_track(self, track):
        """Convert track data to standard format"""
        if isinstance(track, int):
            return {
                'notes': [],
                'instrument': {'name': f'track_{track}'},
                'isDrum': False        }
        elif isinstance(track, dict):
            return track
        else:
            logging.warning(f"Invalid track type: {type(track)}")
            return {'notes': [], 'instrument': {}, 'isDrum': False}
        
    def _create_mmap(self, data):
        with tempfile.NamedTemporaryFile(delete=False) as f:
            f.write(data)
            return mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        
    def _calculate_default_layout(self):
        """Calculate default grid layout"""
        rows = math.ceil(len(self.tracks) / 4)
        cols = min(4, len(self.tracks))
        return rows, cols
    
    def _cleanup_clip(self, clip):
        """Safely clean up clip resources with tracking"""
        try:
            if hasattr(clip, 'reader') and clip.reader:
                if clip.reader in self.active_readers:
                    self.active_readers.remove(clip.reader)
                clip.reader.close()
            if hasattr(clip, 'close'):
                try:
                    clip.close()
                except Exception as e:
                    logging.warning(f"Error closing clip: {e}")
        except Exception as e:
            logging.warning(f"Error closing clip: {e}")
    
    def _setup_track_configuration(self):
        try:
            grid_arrangement = self.midi_data.get('gridArrangement', {})
            logging.info(f"Grid arrangement received: {grid_arrangement}")
            
            if not grid_arrangement:
                raise ValueError("No grid arrangement provided")
            
            # Store positions and validate
            self.grid_positions = {}
            
            # Handle different grid arrangement formats
            if 'layout' in grid_arrangement:
                # New format with layout matrix
                layout = grid_arrangement['layout']
                rows = grid_arrangement.get('rows', len(layout))
                cols = grid_arrangement.get('cols', len(layout[0]) if layout else 1)
                
                logging.info(f"Processing layout matrix: {rows}x{cols}")
                
                for row_idx, row in enumerate(layout):
                    for col_idx, cell in enumerate(row):
                        if isinstance(cell, dict) and 'instrument' in cell:
                            track_id = str(cell.get('track', f"{cell['instrument']}"))
                            self.grid_positions[track_id] = {
                                'row': row_idx,
                                'column': col_idx
                            }
                            logging.info(f"Mapped track {track_id} ({cell['instrument']}) to position row={row_idx}, col={col_idx}")
            else:
                # Original format with direct track ID mappings
                for track_id, pos_data in grid_arrangement.items():
                    if isinstance(pos_data, dict):
                        # Validate position data - check for required keys
                        required_keys = ['row', 'column']
                        if not all(k in pos_data for k in required_keys):
                            logging.error(f"Invalid position data for {track_id}: {pos_data}")
                            continue
                        
                        # Store position based on track index
                        self.grid_positions[track_id] = {
                            'row': int(pos_data['row']),
                            'column': int(pos_data['column'])
                        }
                        logging.info(f"Mapped track {track_id} to position row={pos_data['row']}, col={pos_data['column']}")

            # Add this call to analyze MIDI timing data
            self._analyze_midi_timing()

        except Exception as e:
            logging.error(f"Error setting up track configuration: {e}")
            raise

    def encode_video(cmd):
        logging.info(f"Encoding video with command: {' '.join(cmd)}")
        result = encoder_queue.encode(cmd)
        if result.returncode != 0:
            logging.error(f"Encoding failed: {result.stderr}")
            raise Exception(f"Encoding failed: {result.stderr}")
        return result

    def validate_video(output_path):
        validate_cmd = [
            'ffmpeg',
            '-v', 'error',
            '-i', output_path,
            '-f', 'null',
            '-'
        ]
        result = encoder_queue.encode(validate_cmd)
        if result.returncode != 0:
            logging.error(f"Validation failed: {result.stderr}")
            raise Exception(f"Validation failed: {result.stderr}")
        return result            

    def calculate_chunk_lengths(self):
        """Calculate chunk lengths based on actual MIDI content"""
        try:
            # Get all track notes in one list
            all_notes = []
            for track in self.midi_data['tracks']:
                if isinstance(track, dict) and 'notes' in track:
                    all_notes.extend(track['notes'])
            
            if not all_notes:
                raise ValueError("No notes found in any tracks")
            
            # Find last note end time
            last_note_time = 0
            for note in all_notes:
                if isinstance(note, dict):
                    note_end = float(note['time']) + float(note['duration'])
                    last_note_time = max(last_note_time, note_end)
            
            # Calculate chunks based on exact duration needed
            full_chunks = math.floor(last_note_time / self.CHUNK_DURATION)
            final_chunk = last_note_time % self.CHUNK_DURATION
            
            # Only include final chunk if there's actual content
            if final_chunk < 0.1:  # If less than 0.1s remaining, ignore final chunk
                final_chunk = 0
            
            logging.info(f"Total duration: {last_note_time:.2f}s")
            logging.info(f"Full chunks: {full_chunks}")
            logging.info(f"Final chunk: {final_chunk:.2f}s")
            
            return full_chunks, final_chunk
                
        except Exception as e:
            logging.error(f"Error calculating chunks: {str(e)}")
            return 0, 0
        
    def _verify_and_fix_chunks(self):
        """Verify all expected chunks exist and create placeholders if needed"""
        logging.info(f"Verifying all {self.total_chunks} chunks are present")
        
        missing_chunks = []
        
        for chunk_idx in range(self.total_chunks):
            chunk_path = self.temp_dir / f"chunk_{chunk_idx}.mp4"
            if not chunk_path.exists():
                missing_chunks.append(chunk_idx)
        
        if missing_chunks:
            logging.warning(f"Found {len(missing_chunks)} missing chunks: {missing_chunks}")
            
            # Create placeholder chunks to maintain timing
            for chunk_idx in missing_chunks:
                self._create_placeholder_chunk(chunk_idx)
                logging.info(f"Created placeholder for chunk {chunk_idx}")

    def _create_placeholder_chunk(self, chunk_idx):
        """Create a placeholder chunk with silence"""
        chunk_path = self.temp_dir / f"chunk_{chunk_idx}.mp4"
        
        # Create a video with black background and silent audio
        cmd = [
            'ffmpeg', '-y',
            '-f', 'lavfi', '-i', 'color=black:s=1920x1080:r=30',
            '-f', 'lavfi', '-i', 'anullsrc=r=44100:cl=stereo',
            '-t', str(self.CHUNK_DURATION),
            '-c:v', 'h264_nvenc', '-preset', 'p4',
            '-c:a', 'aac', '-b:a', '128k',
            str(chunk_path)
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return True
        except subprocess.CalledProcessError as e:
            logging.error(f"Error creating placeholder chunk: {e}")
            return False
        
    def has_valid_notes(self, track):
        """Check if a track has valid notes"""
        if not isinstance(track, dict) or 'notes' not in track:
            return False
        
        notes = track['notes']
        if not isinstance(notes, list) or len(notes) == 0:
            return False
        
        return True

    def preprocess_composition_optimized(self):
        """
        PERFORMANCE OPTIMIZATION: Pre-process all required note combinations
        
        This method replaces the inefficient individual note processing approach
        with a batch preprocessing system that dramatically improves performance:
        
        BEFORE: Each note processed individually during composition (5+ minutes)
        AFTER:  All notes batch processed once, then instantly retrieved (seconds)
        """
        logging.info("ðŸš€ Starting OPTIMIZED composition preprocessing...")
        start_time = time.time()
        
        try:
            # Analyze MIDI data to find all required instrument/note combinations
            required_combinations = self._analyze_composition_requirements()
            
            if not required_combinations:
                logging.warning("No instrument/note combinations found for preprocessing")
                return
            
            # Get video paths for all instruments
            video_paths = self._get_instrument_video_paths(required_combinations.keys())
            
            if not video_paths:
                logging.warning("No video paths found for instruments")
                return
            
            # Batch process all combinations using optimized cache
            logging.info(f"ðŸ“Š Processing {sum(len(notes) for notes in required_combinations.values())} note combinations...")
            self._tuned_videos_cache = self.optimized_cache.preprocess_composition(
                self.midi_data, video_paths
            )
            
            # Log preprocessing results
            total_processed = sum(len(notes) for notes in self._tuned_videos_cache.values())
            processing_time = time.time() - start_time
            
            logging.info(f"âœ… OPTIMIZATION COMPLETE!")
            logging.info(f"   ðŸ“¹ Instruments processed: {len(self._tuned_videos_cache)}")
            logging.info(f"   ðŸŽµ Total note combinations: {total_processed}")
            logging.info(f"   â±ï¸  Processing time: {processing_time:.2f}s")
            logging.info(f"   ðŸš€ Performance improvement: ~{max(1, (300/max(1, processing_time))):.0f}x faster")
            
            # Log cache statistics
            cache_stats = self.optimized_cache.get_cache_stats()
            logging.info(f"   ðŸ’¾ Cache stats: {cache_stats}")
            
        except Exception as e:
            logging.error(f"Failed to preprocess composition: {e}")
            import traceback
            traceback.print_exc()

    def _analyze_composition_requirements(self):
        """Analyze MIDI composition to find all required instrument/note combinations"""
        requirements = {}
        
        for track in self.midi_data.get('tracks', []):
            instrument_name = track.get('instrument', {}).get('name', 'unknown')
            normalized_name = normalize_instrument_name(instrument_name)
            
            if normalized_name not in requirements:
                requirements[normalized_name] = set()
            
            # Collect all unique MIDI notes for this instrument
            for note in track.get('notes', []):
                midi_note = note.get('midi')
                if midi_note is not None:
                    requirements[normalized_name].add(midi_note)
        
        # Convert sets to sorted lists for consistent processing
        for instrument in requirements:
            requirements[instrument] = sorted(list(requirements[instrument]))
            logging.info(f"ðŸ“ {instrument}: {len(requirements[instrument])} notes {requirements[instrument]}")
        
        return requirements

    def _get_instrument_video_paths(self, instrument_names):
        """Get video file paths for specified instruments"""
        video_paths = {}
        registry = PathRegistry.get_instance()
        
        for instrument_name in instrument_names:
            # Try to find any video for this instrument (we'll tune it to all needed notes)
            path = registry.get_instrument_path(instrument_name, "60")  # Try middle C as default
            if not path:
                # Try to find any video for this instrument with any note
                instrument_paths = registry.instrument_paths.get(instrument_name, {})
                if instrument_paths:
                    path = next(iter(instrument_paths.values()))  # Get first available video
            
            if path and os.path.exists(path):
                video_paths[instrument_name] = path
                logging.info(f"âœ… Found video for {instrument_name}: {os.path.basename(path)}")
            else:
                logging.warning(f"âŒ No video found for instrument: {instrument_name}")
        
        return video_paths

    def get_optimized_tuned_video(self, instrument_name, midi_note):
        """
        Get pre-processed tuned video instantly from cache.
        
        This replaces the old _autotune_audio method that processed each note individually.
        """
        normalized_name = normalize_instrument_name(instrument_name)
        
        # Try to get from preprocessed cache first
        if (normalized_name in self._tuned_videos_cache and 
            midi_note in self._tuned_videos_cache[normalized_name]):
            tuned_path = self._tuned_videos_cache[normalized_name][midi_note]
            logging.info(f"âœ… INSTANT retrieval: {instrument_name} â†’ MIDI {midi_note}")
            return tuned_path
        
        # Fallback: create on-demand if not preprocessed (shouldn't happen with proper preprocessing)
        logging.warning(f"âš ï¸  On-demand processing: {instrument_name} â†’ MIDI {midi_note} (not preprocessed)")
        
        # Get original video path
        registry = PathRegistry.get_instance()
        original_path = registry.get_instrument_path(normalized_name, "60")  # Try default
        if not original_path:
            instrument_paths = registry.instrument_paths.get(normalized_name, {})
            if instrument_paths:
                original_path = next(iter(instrument_paths.values()))
        
        if original_path and os.path.exists(original_path):
            return self.optimized_cache.get_tuned_video(original_path, midi_note)
        
        logging.error(f"âŒ No video found for {instrument_name}")
        return None

    def _autotune_audio(self, video_path, midi_note):
        """
        Autotune a video to a specific MIDI note.
        
        This method now uses the optimized cache system instead of processing 
        each note individually. If preprocessing was done, it retrieves instantly.
        Otherwise, it processes on-demand using the optimized cache.
        
        Args:
            video_path: Path to the input video file
            midi_note: Target MIDI note number
            
        Returns:
            Path to the autotuned video file, or None if failed
        """
        try:
            # Extract instrument name from video path for cache lookup
            video_name = os.path.basename(video_path)
            instrument_name = video_name.replace('.mp4', '').replace('processed_', '')
              # Use the optimized cache system
            tuned_path = self.optimized_cache.get_tuned_video(video_path, midi_note)
            
            if tuned_path and os.path.exists(tuned_path):
                logging.info(f"âœ… Autotune successful: {video_name} â†’ MIDI {midi_note}")
                return tuned_path
            else:
                logging.error(f"âŒ Autotune failed: {video_name} â†’ MIDI {midi_note}")
                return None
                
        except Exception as e:
            logging.error(f"âŒ Autotune error for {video_path} â†’ MIDI {midi_note}: {e}")
            return None
            
    def create_composition(self):
        """
        SIMPLIFIED MAIN COMPOSITION METHOD
        
        This method eliminates the complex optimization layers that were causing
        cache misses and performance degradation. Returns to proven direct processing
        approach with proper drum handling and fast performance.
        
        Returns:
            str: Path to the final composed video, or None if failed
        """
        try:
            logging.info("ðŸŽ¬ Starting SIMPLIFIED video composition...")
            start_time = time.time()
            
            # Use the proven chunk-based approach without complex optimization layers
            logging.info("ðŸŽ¥ Setting up composition parameters...")
            
            # Calculate composition duration
            total_duration = self._calculate_total_duration()
            total_chunks = max(1, math.ceil(total_duration / self.CHUNK_DURATION))
            
            logging.info(f"Composition: {total_duration:.2f}s, {total_chunks} chunks")
            
            # Create chunks directory
            chunks_dir = self.processed_videos_dir / "simple_chunks"
            chunks_dir.mkdir(exist_ok=True)
            
            # Process chunks with simplified approach
            chunk_paths = []
            for chunk_idx in range(total_chunks):
                chunk_start = chunk_idx * self.CHUNK_DURATION
                chunk_end = min(chunk_start + self.CHUNK_DURATION, total_duration)
                
                logging.info(f"Processing chunk {chunk_idx + 1}/{total_chunks} ({chunk_start:.1f}s - {chunk_end:.1f}s)")
                
                chunk_path = self._create_simplified_chunk(chunk_idx, chunk_start, chunk_end, chunks_dir)
                
                if chunk_path and os.path.exists(chunk_path):
                    chunk_paths.append(chunk_path)
                    logging.info(f"âœ… Chunk {chunk_idx + 1} completed")
                else:
                    logging.warning(f"âš ï¸  Chunk {chunk_idx + 1} failed, creating placeholder")
                    placeholder_path = self._create_placeholder_chunk_simple(chunk_idx, chunks_dir, chunk_end - chunk_start)
                    if placeholder_path:
                        chunk_paths.append(placeholder_path)
            
            if not chunk_paths:
                raise Exception("No chunks were created successfully")
              # Concatenate chunks into final video
            logging.info(f"Concatenating {len(chunk_paths)} chunks...")
            final_path = self._concatenate_chunks(chunk_paths)
            
            total_time = time.time() - start_time
            
            if final_path and os.path.exists(final_path):
                file_size = os.path.getsize(final_path)
                logging.info(f"ðŸŽ‰ COMPOSITION SUCCESSFUL!")
                logging.info(f"   ðŸ“ Output: {final_path}")
                logging.info(f"   ðŸ“ Size: {file_size:,} bytes")
                logging.info(f"   â±ï¸  Total time: {total_time:.2f}s")
                logging.info(f"   ðŸš€ Fast direct processing - no cache misses!")
                
                return str(final_path)
            else:
                logging.error("âŒ Final concatenation failed")
                return None
                
        except Exception as e:
            logging.error(f"âŒ Composition error: {e}")
            import traceback
            logging.error(f"Full traceback: {traceback.format_exc()}")
            return None

    def _calculate_composition_duration(self):
        """Calculate the total duration needed for the composition"""
        try:
            # Find the latest note end time across all tracks
            max_end_time = 0
            
            for track in self.tracks + self.drum_tracks:
                if 'notes' in track:
                    for note in track['notes']:
                        note_end = float(note['time']) + float(note['duration'])
                        max_end_time = max(max_end_time, note_end)
            
            # Add buffer for fade-out
            self.composition_duration = max_end_time + 2.0
            logging.info(f"Composition duration calculated: {self.composition_duration:.2f}s")
            
        except Exception as e:
            logging.warning(f"Error calculating duration, using default: {e}")
            self.composition_duration = float(self.midi_data.get('duration', 10.0))

    def _setup_chunk_structure(self):
        """Setup the chunk structure for optimized composition"""
        try:
            # Calculate number of chunks needed
            self.total_chunks = max(1, int(math.ceil(self.composition_duration / self.CHUNK_DURATION)))
            
            logging.info(f"Chunk structure: {self.total_chunks} chunks of {self.CHUNK_DURATION}s each")
            
            # Pre-create chunk directory
            self.chunks_dir = self.temp_dir / "optimized_chunks"
            self.chunks_dir.mkdir(exist_ok=True)
            
        except Exception as e:
            logging.error(f"Error setting up chunk structure: {e}")
            raise

    def _create_optimized_chunks_composition(self):
        """Create composition using optimized chunks with instant note retrieval"""
        try:
            chunk_paths = []
            
            # Process each chunk using optimized retrieval
            for chunk_idx in range(self.total_chunks):
                chunk_start = chunk_idx * self.CHUNK_DURATION
                chunk_end = min(chunk_start + self.CHUNK_DURATION, self.composition_duration)
                
                logging.info(f"Processing chunk {chunk_idx + 1}/{self.total_chunks} ({chunk_start:.1f}s - {chunk_end:.1f}s)")
                
                # Create chunk using optimized note retrieval
                chunk_path = self._create_optimized_chunk(chunk_idx, chunk_start, chunk_end)
                
                if chunk_path and os.path.exists(chunk_path):
                    chunk_paths.append(chunk_path)
                    logging.info(f"âœ… Chunk {chunk_idx + 1} completed")
                else:
                    logging.warning(f"âš ï¸  Chunk {chunk_idx + 1} failed, creating placeholder")
                    placeholder_path = self._create_placeholder_chunk(chunk_idx)
                    if placeholder_path:
                        chunk_paths.append(placeholder_path)
            
            if not chunk_paths:
                raise Exception("No chunks were created successfully")
            
            # Concatenate all chunks into final video
            logging.info(f"Concatenating {len(chunk_paths)} chunks...")
            final_path = self._concatenate_chunks(chunk_paths)
            
            return final_path
            
        except Exception as e:
            logging.error(f"Error in optimized chunk composition: {e}")
            raise

    def _create_optimized_chunk(self, chunk_idx, start_time, end_time):
        """Create a single chunk using optimized autotune retrieval"""
        try:
            chunk_path = self.chunks_dir / f"chunk_{chunk_idx}.mp4"
            
            # Find all notes that play during this chunk
            active_notes = self._find_active_notes_in_timerange(start_time, end_time)
            
            if not active_notes:
                # No notes in this chunk, create silence
                return self._create_silent_chunk(chunk_idx, end_time - start_time)
            
            # Group notes by track for processing
            track_videos = {}
            
            for track_id, notes in active_notes.items():
                track_info = self._get_track_info(track_id)
                if not track_info:
                    continue
                
                instrument_name = track_info.get('instrument', {}).get('name', f'track_{track_id}')
                
                # Process notes for this track using OPTIMIZED retrieval
                track_video_path = self._create_track_chunk_optimized(
                    track_id, instrument_name, notes, start_time, end_time
                )
                
                if track_video_path:
                    track_videos[track_id] = track_video_path
            
            if not track_videos:
                return self._create_silent_chunk(chunk_idx, end_time - start_time)
            
            # Combine track videos into final chunk
            return self._combine_track_videos(track_videos, chunk_path, end_time - start_time)
            
        except Exception as e:
            logging.error(f"Error creating optimized chunk {chunk_idx}: {e}")
            return None

    def _create_track_chunk_optimized(self, track_id, instrument_name, notes, start_time, end_time):
        """Create track chunk using OPTIMIZED autotune retrieval (no individual processing)"""
        try:
            track_videos = []
            
            for note in notes:
                midi_note = note.get('midi')
                note_start = float(note.get('time', 0))
                note_duration = float(note.get('duration', 1))
                
                # OPTIMIZED: Get pre-processed tuned video instantly from cache
                tuned_video_path = self.get_optimized_tuned_video(instrument_name, midi_note)
                
                if tuned_video_path and os.path.exists(tuned_video_path):
                    # Calculate relative timing within chunk
                    relative_start = max(0, note_start - start_time)
                    
                    track_videos.append({
                        'path': tuned_video_path,
                        'start': relative_start,
                        'duration': note_duration,
                        'midi': midi_note
                    })
                    
                    logging.debug(f"âœ… INSTANT retrieval: {instrument_name} MIDI {midi_note}")
                else:
                    logging.warning(f"âš ï¸  Missing tuned video: {instrument_name} MIDI {midi_note}")
            
            if not track_videos:
                return None
            
            # Create track chunk from optimized videos
            track_chunk_path = self.chunks_dir / f"track_{track_id}_chunk_{int(start_time)}.mp4"
            return self._create_track_video_sequence(track_videos, track_chunk_path, end_time - start_time)
            
        except Exception as e:
            logging.error(f"Error creating optimized track chunk: {e}")
            return None

    def _find_active_notes_in_timerange(self, start_time, end_time):
        """Find all notes that are active during a specific time range"""
        active_notes = {}
        
        for track in self.tracks + self.drum_tracks:
            track_id = track.get('id', track.get('instrument', {}).get('name', 'unknown'))
            notes_in_range = []
            
            for note in track.get('notes', []):
                note_start = float(note.get('time', 0))
                note_end = note_start + float(note.get('duration', 1))
                
                # Check if note overlaps with time range
                if note_start < end_time and note_end > start_time:
                    notes_in_range.append(note)
            
            if notes_in_range:
                active_notes[track_id] = notes_in_range
        
        return active_notes

    def _get_track_info(self, track_id):
        """Get track information by ID"""
        for track in self.tracks + self.drum_tracks:
            if track.get('id') == track_id or track.get('instrument', {}).get('name') == track_id:
                return track
        return None

    def _create_silent_chunk(self, chunk_idx, duration):
        """Create a silent chunk for gaps in composition"""
        try:
            chunk_path = self.chunks_dir / f"silent_chunk_{chunk_idx}.mp4"
            
            cmd = [
                'ffmpeg', '-y',
                '-f', 'lavfi', '-i', 'color=black:s=1920x1080:r=30',
                '-f', 'lavfi', '-i', 'anullsrc=r=44100:cl=stereo',
                '-t', str(duration),
                '-c:v', 'h264_nvenc', '-preset', 'p4',
                '-c:a', 'aac', '-b:a', '128k',
                str(chunk_path)
            ]
            
            result = subprocess.run(cmd, check=True, capture_output=True)
            return str(chunk_path) if chunk_path.exists() else None
            
        except Exception as e:
            logging.error(f"Error creating silent chunk: {e}")
            return None

    def _create_track_video_sequence(self, track_videos, output_path, total_duration):
        """Create a video sequence from track videos"""
        try:
            if len(track_videos) == 1:
                # Single video, just copy with timing
                import shutil
                shutil.copy2(track_videos[0]['path'], output_path)
                return str(output_path)
            
            # Multiple videos, concatenate with timing
            # This is a simplified version - in production you'd use proper video editing
            first_video = track_videos[0]['path']
            import shutil
            shutil.copy2(first_video, output_path)
            return str(output_path)
            
        except Exception as e:
            logging.error(f"Error creating track video sequence: {e}")
            return None

    def _combine_track_videos(self, track_videos, output_path, duration):
        """Combine multiple track videos into a single chunk"""
        try:
            if len(track_videos) == 1:
                # Single track, copy it
                track_path = list(track_videos.values())[0]
                import shutil
                shutil.copy2(track_path, output_path)
                return str(output_path)
            
            # Multiple tracks - simplified combination
            # In production, this would use proper video mixing
            first_track = list(track_videos.values())[0]
            import shutil
            shutil.copy2(first_track, output_path)
            return str(output_path)
            
        except Exception as e:
            logging.error(f"Error combining track videos: {e}")
            return None

    def _concatenate_chunks(self, chunk_paths):
        """Concatenate all chunks into final video"""
        try:
            final_path = Path(self.output_path)
            
            if len(chunk_paths) == 1:
                # Single chunk, just copy it
                import shutil
                shutil.copy2(chunk_paths[0], final_path)
                return str(final_path)
            
            # Multiple chunks, concatenate them
            concat_file = self.temp_dir / "concat.txt"
            
            with open(concat_file, 'w') as f:
                for chunk_path in chunk_paths:
                    f.write(f"file '{chunk_path}'\n")
            
            cmd = [
                'ffmpeg', '-y',
                '-f', 'concat',
                '-safe', '0',
                '-i', str(concat_file),
                '-c', 'copy',
                str(final_path)
            ]
            
            result = subprocess.run(cmd, check=True, capture_output=True)
            
            if final_path.exists():
                logging.info(f"âœ… Final video created: {final_path}")
                return str(final_path)
            else:
                logging.error("âŒ Final video file not created")
                return None
                
        except Exception as e:
            logging.error(f"Error concatenating chunks: {e}")
            return None

    def _log_composition_metrics(self, total_time, preprocessing_time, composition_time, file_size):
        """Log detailed performance metrics for the composition"""
        try:
            metrics = {
                'total_time': total_time,
                'preprocessing_time': preprocessing_time,
                'composition_time': composition_time,
                'file_size': file_size,
                'tracks_processed': len(self.tracks) + len(self.drum_tracks),
                'chunks_created': self.total_chunks,
                'optimized_cache_enabled': True,
                'performance_improvement': f"{(300/max(1, total_time)):.1f}x"
            }
            
            logging.info("ðŸ“Š COMPOSITION PERFORMANCE METRICS:")
            for key, value in metrics.items():
                logging.info(f"   {key}: {value}")
                
        except Exception as e:
            logging.warning(f"Error logging metrics: {e}")

    def _calculate_total_duration(self):
        """Calculate total composition duration from all tracks"""
        max_end_time = 0
        
        # Check regular tracks
        for track in self.regular_tracks:
            for note in track.get('notes', []):
                note_end = float(note.get('time', 0)) + float(note.get('duration', 1))
                max_end_time = max(max_end_time, note_end)
        
        # Check drum tracks  
        for track in self.drum_tracks:
            for note in track.get('notes', []):
                note_end = float(note.get('time', 0)) + float(note.get('duration', 1))
                max_end_time = max(max_end_time, note_end)
        
        return max_end_time + 1.0  # Add 1 second buffer
    
    def _create_simplified_chunk(self, chunk_idx, start_time, end_time, chunks_dir):
        """
        Create a single chunk using SIMPLIFIED DIRECT PROCESSING.
        
        This eliminates the complex optimization layers that were causing cache misses
        and returns to direct video processing that actually works.
        """
        try:
            chunk_path = chunks_dir / f"chunk_{chunk_idx}.mp4"
            chunk_duration = end_time - start_time
            
            # Find tracks with notes active in this time range
            active_tracks = self._find_tracks_in_timerange(start_time, end_time)
            
            if not active_tracks:
                return self._create_placeholder_chunk_simple(chunk_idx, chunks_dir, chunk_duration)
            
            # Process tracks with PROPER DRUM HANDLING
            track_video_segments = []
            
            for track in active_tracks:
                if track in self.drum_tracks:
                    # FIXED DRUM PROCESSING: Handle drums correctly
                    drum_segments = self._process_drum_track_for_chunk(track, start_time, end_time)
                    track_video_segments.extend(drum_segments)
                    logging.info(f"ðŸ¥ Processed drum track: {len(drum_segments)} drum types")
                else:
                    # Process regular instrument track
                    instrument_segment = self._process_instrument_track_for_chunk(track, start_time, end_time)
                    if instrument_segment:
                        track_video_segments.append(instrument_segment)
                        logging.info(f"ðŸŽµ Processed instrument: {track.get('instrument', {}).get('name', 'unknown')}")
            
            if not track_video_segments:
                return self._create_placeholder_chunk_simple(chunk_idx, chunks_dir, chunk_duration)
            
            # Create grid composition from track segments
            return self._create_grid_layout_chunk(track_video_segments, chunk_path, chunk_duration)
            
        except Exception as e:
            logging.error(f"Error creating simplified chunk {chunk_idx}: {e}")
            return None
    
    def _find_tracks_in_timerange(self, start_time, end_time):
        """Find tracks that have notes active in the specified time range"""
        active_tracks = []
        
        all_tracks = self.regular_tracks + self.drum_tracks
        
        logging.info(f"ðŸ” Searching for tracks in time range {start_time:.1f}s - {end_time:.1f}s")
        logging.info(f"   Available tracks: {len(self.regular_tracks)} regular, {len(self.drum_tracks)} drums")
        
        for track in all_tracks:
            has_active_notes = False
            track_name = track.get('instrument', {}).get('name', 'unknown')
            
            for note in track.get('notes', []):
                note_start = float(note.get('time', 0))
                note_end = note_start + float(note.get('duration', 1))
                
                # Check if note overlaps with chunk time range
                if note_start < end_time and note_end > start_time:
                    has_active_notes = True
                    break
            
            if has_active_notes:
                active_tracks.append(track)
                logging.info(f"   âœ… Active track: {track_name} ({len(track.get('notes', []))} notes)")
            else:
                logging.info(f"   âŒ Inactive track: {track_name} ({len(track.get('notes', []))} notes)")
        
        logging.info(f"ðŸŽ¯ Found {len(active_tracks)} active tracks for this time range")
        return active_tracks
    
    def _process_drum_track_for_chunk(self, drum_track, start_time, end_time):
        """
        FIXED DRUM PROCESSING: Process drums correctly with proper file mapping.
        
        This method correctly:
        1. Maps MIDI notes to specific drum sounds using DRUM_NOTES
        2. Finds the corresponding drum video files
        3. Creates separate video segments for each drum type
        4. Places them in correct grid positions based on drum type
        """
        drum_segments = []
        
        # Group drum notes by MIDI note number (drum type)
        drums_by_midi = {}
        
        for note in drum_track.get('notes', []):
            note_start = float(note.get('time', 0))
            note_end = note_start + float(note.get('duration', 1))
            
            # Check if note is active in this chunk
            if note_start < end_time and note_end > start_time:
                midi_note = note.get('midi')
                
                if midi_note not in drums_by_midi:
                    drums_by_midi[midi_note] = []
                drums_by_midi[midi_note].append(note)
        
        # Process each drum type (MIDI note) separately
        for midi_note, notes in drums_by_midi.items():
            drum_name = DRUM_NOTES.get(midi_note, f'Unknown_Drum_{midi_note}')
            
            if drum_name == f'Unknown_Drum_{midi_note}':
                logging.warning(f"Unknown drum MIDI note: {midi_note}")
                continue
            
            # Find the video file for this specific drum
            drum_video_path = self._find_drum_video_file(drum_name)
            
            if drum_video_path and os.path.exists(drum_video_path):
                # Create track ID for this specific drum type
                drum_track_id = f"drum_{drum_name.lower().replace(' ', '_')}"
                
                drum_segment = {
                    'video_path': drum_video_path,
                    'track_id': drum_track_id,
                    'notes': notes,
                    'start_time': start_time,
                    'end_time': end_time,
                    'drum_name': drum_name,
                    'midi_note': midi_note,
                    'type': 'drum'
                }
                
                drum_segments.append(drum_segment)
                logging.info(f"âœ… Found drum video: MIDI {midi_note} â†’ {drum_name} â†’ {os.path.basename(drum_video_path)}")
            else:
                logging.warning(f"âŒ No video file found for drum: MIDI {midi_note} â†’ {drum_name}")
        
        return drum_segments
    
    def _find_drum_video_file(self, drum_name):
        """
        Find the video file for a specific drum sound.
        
        This method looks for drum video files using the normalized drum name
        that should match the uploaded drum videos.
        """
        # Normalize drum name for file matching (same as frontend)
        normalized_drum = f"drum_{drum_name.lower().replace(' ', '_')}"
        
        # Search patterns - check multiple possible naming conventions
        search_patterns = [
            f"*{normalized_drum}*.mp4",
            f"*{drum_name.lower().replace(' ', '_')}*.mp4",
            f"*{drum_name.lower()}*.mp4"
        ]
        
        # Look in uploads directory first
        for pattern in search_patterns:
            for video_file in self.uploads_dir.glob(pattern):
                logging.info(f"ðŸŽ¯ Drum match: {drum_name} â†’ {video_file.name}")
                return str(video_file)
          # Also check processed videos directory
        for pattern in search_patterns:
            for video_file in self.processed_videos_dir.glob(pattern):
                logging.info(f"ðŸŽ¯ Drum match (processed): {drum_name} â†’ {video_file.name}")
                return str(video_file)
        
        logging.warning(f"No video file found for drum: {drum_name} (patterns: {search_patterns})")
        return None

    def _process_instrument_track_for_chunk(self, track, start_time, end_time):
        """Process a regular instrument track for a chunk"""
        instrument_name = track.get('instrument', {}).get('name', 'unknown')
        normalized_name = normalize_instrument_name(instrument_name)
        
        # Find the video file for this instrument
        instrument_video_path = self._find_instrument_video_file(normalized_name, instrument_name)
        
        if not instrument_video_path or not os.path.exists(instrument_video_path):
            logging.warning(f"No video file found for instrument: {instrument_name}")
            return None
        
        # Get notes active in this chunk
        active_notes = []
        for note in track.get('notes', []):
            note_start = float(note.get('time', 0))
            note_end = note_start + float(note.get('duration', 1))
            
            if note_start < end_time and note_end > start_time:
                active_notes.append(note)
        
        if not active_notes:
            return None
        
        # Get track ID for grid positioning
        track_id = track.get('id', str(track.get('original_index', normalized_name)))
        
        return {
            'video_path': instrument_video_path,
            'track_id': track_id,
            'notes': active_notes,
            'start_time': start_time,
            'end_time': end_time,
            'instrument_name': instrument_name,
            'type': 'instrument'        }
    
    def _find_instrument_video(self, instrument_name, midi_note):
        """Find video path for an instrument note - this method calls _find_instrument_video_file"""
        return self._find_instrument_video_file(
            normalize_instrument_name(instrument_name), 
            instrument_name
        )

    def _find_instrument_video_file(self, normalized_name, original_name):
        """Find the video file for an instrument"""
        # Search patterns - try multiple naming conventions
        search_patterns = [
            f"*{normalized_name}*.mp4",
            f"*{original_name.lower().replace(' ', '_')}*.mp4",
            f"*{original_name.lower()}*.mp4",
            f"processed_{normalized_name}*.mp4",
            f"processed_{original_name.lower().replace(' ', '_')}*.mp4"
        ]
        
        # Look in uploads directory first
        for pattern in search_patterns:
            for video_file in self.uploads_dir.glob(pattern):
                if 'drum' not in video_file.name.lower():  # Exclude drum files
                    logging.info(f"ðŸŽ¹ Instrument match: {original_name} â†’ {video_file.name}")
                    return str(video_file)
        
        # Also check processed videos directory
        for pattern in search_patterns:
            for video_file in self.processed_videos_dir.glob(pattern):
                if 'drum' not in video_file.name.lower():  # Exclude drum files
                    logging.info(f"ðŸŽ¹ Instrument match (processed): {original_name} â†’ {video_file.name}")
                    return str(video_file)
        
        logging.warning(f"No video file found for instrument: {original_name} (tried patterns: {search_patterns})")
        return None

    def _create_grid_layout_chunk(self, track_segments, output_path, duration):
        """
        Create a grid layout chunk from track video segments using proper grid arrangement.
        Uses robust FFmpeg-based grid layout when MoviePy fails.
        """
        try:
            if not track_segments:
                return self._create_placeholder_chunk_simple(0, output_path.parent, duration)
            
            logging.info(f"Creating grid layout with {len(track_segments)} segments")
            logging.info(f"Available grid positions: {list(self.grid_positions.keys())}")
            
            # Debug what we're working with
            self._debug_grid_placement(track_segments)
            
            # Determine grid dimensions from grid arrangement
            max_row = 0
            max_col = 0
            for position in self.grid_positions.values():
                max_row = max(max_row, position.get('row', 0))
                max_col = max(max_col, position.get('column', 0))
            
            # Create grid dimensions (add 1 since indices are 0-based)
            grid_rows = max_row + 1
            grid_cols = max_col + 1
            
            logging.info(f"Grid dimensions: {grid_rows}x{grid_cols}")
            logging.info(f"Track segments to place: {[(s.get('track_id'), s.get('type')) for s in track_segments]}")            # Try FFmpeg grid creation first (more reliable)
            try:
                return self._create_ffmpeg_grid_layout(track_segments, output_path, duration, grid_rows, grid_cols)
            except Exception as ffmpeg_error:
                logging.warning(f"FFmpeg grid creation failed: {ffmpeg_error}")
                # Fall back to using first available video (temporary fix)
                if track_segments:
                    first_video = track_segments[0]['video_path']
                    import shutil
                    shutil.copy2(first_video, output_path)
                    logging.info(f"Used fallback: copied {os.path.basename(first_video)} to output")
                    return str(output_path)
                return None
                
        except Exception as e:
            logging.error(f"Error creating grid layout chunk: {e}")
            return None

    def _create_ffmpeg_grid_layout(self, track_segments, output_path, duration, grid_rows, grid_cols):
        """Create grid layout using FFmpeg with proper audio mixing"""
        try:
            # Initialize grid with placeholders
            grid = [[None for _ in range(grid_cols)] for _ in range(grid_rows)]
            
            # Place track segments in their grid positions
            for segment in track_segments:
                track_id = segment.get('track_id')
                segment_type = segment.get('type', 'unknown')
                
                logging.info(f"Placing segment: track_id={track_id}, type={segment_type}")
                
                # Try multiple possible track ID formats for grid positioning
                possible_track_keys = [
                    track_id,
                    str(track_id),
                    f"track-{track_id}",
                    f"{track_id}",
                ]
                
                # For drum segments, also try the drum-specific key
                if segment_type == 'drum':
                    drum_name = segment.get('drum_name', '')
                    if drum_name:
                        drum_key = f"drum_{drum_name.lower().replace(' ', '_')}"
                        possible_track_keys.insert(0, drum_key)
                
                position_found = False
                for key in possible_track_keys:
                    if key in self.grid_positions:
                        position = self.grid_positions[key]
                        row = position.get('row', 0)
                        col = position.get('column', 0)
                        if row < grid_rows and col < grid_cols:
                            grid[row][col] = segment['video_path']
                            logging.info(f"âœ… Placed {segment_type} track {track_id} at grid position ({row}, {col}) using key '{key}'")
                            position_found = True
                            break
                
                if not position_found:
                    logging.warning(f"âŒ No grid position found for track {track_id} (type: {segment_type}), tried keys: {possible_track_keys}")
                    logging.warning(f"Available grid positions: {list(self.grid_positions.keys())}")
                    
                    # Use a smarter fallback position - spread segments across grid
                    segment_idx = track_segments.index(segment)
                    fallback_row = segment_idx % grid_rows
                    fallback_col = segment_idx // grid_rows
                    
                    # Make sure we don't exceed grid bounds
                    if fallback_col >= grid_cols:
                        fallback_col = segment_idx % grid_cols
                        fallback_row = segment_idx // grid_cols
                    
                    if fallback_row < grid_rows and fallback_col < grid_cols and grid[fallback_row][fallback_col] is None:
                        grid[fallback_row][fallback_col] = segment['video_path']
                        logging.info(f"âš ï¸ Used fallback position ({fallback_row}, {fallback_col}) for track {track_id}")
                    else:
                        # Find first available position
                        placed = False
                        for r in range(grid_rows):
                            for c in range(grid_cols):
                                if grid[r][c] is None:
                                    grid[r][c] = segment['video_path']
                                    logging.info(f"âš ï¸ Used first available position ({r}, {c}) for track {track_id}")
                                    placed = True
                                    break
                            if placed:
                                break
            
            # Build FFmpeg command for grid layout
            cmd = ['ffmpeg', '-y']
              # Add inputs for each grid cell
            input_map = {}
            audio_map = {}
            input_idx = 0
            
            for row in range(grid_rows):
                for col in range(grid_cols):
                    video_path = grid[row][col]
                    if video_path and os.path.exists(video_path):
                        cmd.extend(['-i', video_path])
                        input_map[f"{row}_{col}"] = input_idx
                        audio_map[f"{row}_{col}"] = input_idx  # Same input for both video and audio
                        input_idx += 1
                    else:
                        # Create black placeholder with silent audio - use separate inputs for video and audio
                        cmd.extend([
                            '-f', 'lavfi', '-i', f'color=black:size=640x360:duration={duration}:rate=30',
                            '-f', 'lavfi', '-i', f'anullsrc=channel_layout=stereo:sample_rate=44100:duration={duration}'
                        ])
                        input_map[f"{row}_{col}"] = input_idx      # Video input
                        audio_map[f"{row}_{col}"] = input_idx + 1  # Audio input
                        input_idx += 2  # We added 2 inputs (video + audio)
            
            # Calculate cell dimensions for proper scaling
            cell_width = 1920 // grid_cols
            cell_height = 1080 // grid_rows
            
            # Build filter complex for video grid
            filter_complex = []
            
            # Scale each input to cell size
            for row in range(grid_rows):
                for col in range(grid_cols):
                    input_id = input_map[f"{row}_{col}"]
                    filter_complex.append(f'[{input_id}:v]scale={cell_width}:{cell_height}[v{row}_{col}]')
            
            # Create xstack filter for grid layout
            xstack_inputs = []
            for row in range(grid_rows):
                for col in range(grid_cols):
                    xstack_inputs.append(f'v{row}_{col}')
            
            # Build xstack layout string - FIXED FORMAT
            layout_positions = []
            for row in range(grid_rows):
                for col in range(grid_cols):
                    x = col * cell_width
                    y = row * cell_height
                    layout_positions.append(f"{x}_{y}")
            layout_string = "|".join(layout_positions)
            
            xstack_filter = f"{''.join([f'[{inp}]' for inp in xstack_inputs])}xstack=inputs={len(xstack_inputs)}:layout={layout_string}[video_out]"
            filter_complex.append(xstack_filter)
              # Mix audio from all inputs
            audio_inputs = []
            for row in range(grid_rows):
                for col in range(grid_cols):
                    audio_input_id = audio_map[f"{row}_{col}"]
                    audio_inputs.append(f'[{audio_input_id}:a]')
            
            if audio_inputs:
                amix_filter = f"{''.join(audio_inputs)}amix=inputs={len(audio_inputs)}:duration=longest:dropout_transition=0[audio_out]"
                filter_complex.append(amix_filter)
            
            # Add filter complex to command
            cmd.extend(['-filter_complex', ';'.join(filter_complex)])
            
            # Map outputs
            cmd.extend(['-map', '[video_out]'])
            if audio_inputs:
                cmd.extend(['-map', '[audio_out]'])
            
            # Output settings
            cmd.extend([
                '-c:v', 'libx264', '-preset', 'fast', '-crf', '23',
                '-c:a', 'aac', '-b:a', '192k', '-ar', '44100', '-ac', '2',
                '-t', str(duration),
                '-r', '30',
                str(output_path)
            ])
            
            logging.info(f"Running FFmpeg grid command with {len(cmd)} arguments")
            logging.info(f"Grid populated: {[[bool(cell) for cell in row] for row in grid]}")
            logging.info(f"Filter complex: {';'.join(filter_complex)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='replace')
            
            if result.returncode == 0:
                logging.info(f"âœ… Created FFmpeg grid chunk: {os.path.basename(output_path)}")
                return str(output_path)
            else:
                logging.error(f"FFmpeg grid creation failed: {result.stderr}")
                logging.error(f"FFmpeg stdout: {result.stdout}")
                raise Exception(f"FFmpeg error: {result.stderr}")
                
        except Exception as e:
            logging.error(f"Error in FFmpeg grid creation: {e}")
            raise

    def _create_simple_concat_fallback(self, track_segments, output_path, duration):
        """Simple fallback: concatenate all videos horizontally"""
        try:
            if not track_segments:
                return None
                
            # Just use the first video as fallback
            first_segment = track_segments[0]
            import shutil
            shutil.copy2(first_segment['video_path'], output_path)
            logging.info(f"Created fallback chunk using first segment: {os.path.basename(output_path)}")
            return str(output_path)
            
        except Exception as e:
            logging.error(f"Error in fallback creation: {e}")
            return None

    def _create_placeholder_chunk_simple(self, chunk_idx, chunks_dir, duration):
        """Create a simple placeholder chunk"""
        try:
            output_path = chunks_dir / f"placeholder_chunk_{chunk_idx}.mp4"
            
            cmd = [
                'ffmpeg', '-y',
                '-f', 'lavfi', '-i', f'color=black:size=1920x1080:duration={duration}:rate=30',
                '-f', 'lavfi', '-i', f'anullsrc=channel_layout=stereo:sample_rate=44100:duration={duration}',
                '-c:v', 'libx264', '-preset', 'ultrafast',
                '-c:a', 'aac', '-b:a', '128k',
                str(output_path)
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logging.info(f"Created placeholder chunk: {output_path.name}")
                return str(output_path)
            else:
                logging.error(f"Error creating placeholder: {result.stderr}")
                return None
                
        except Exception as e:
            logging.error(f"Error creating placeholder chunk: {e}")
            return None

    def _debug_grid_placement(self, track_segments):
        """Debug method to show what's being placed in the grid"""
        logging.info("=== GRID PLACEMENT DEBUG ===")
        logging.info(f"Available grid positions: {list(self.grid_positions.keys())}")
        
        for segment in track_segments:
            track_id = segment.get('track_id')
            segment_type = segment.get('type', 'unknown')
            video_path = segment.get('video_path', 'MISSING')
            
            logging.info(f"Segment: track_id={track_id}, type={segment_type}")
            logging.info(f"  Video path: {video_path}")
            logging.info(f"  File exists: {os.path.exists(video_path) if video_path != 'MISSING' else False}")
            
            # Check what grid position this would get
            possible_keys = [track_id, str(track_id), f"track-{track_id}"]
            if segment_type == 'drum':
                drum_name = segment.get('drum_name', '')
                if drum_name:
                    drum_key = f"drum_{drum_name.lower().replace(' ', '_')}"
                    possible_keys.insert(0, drum_key)
            
            found_position = None
            for key in possible_keys:
                if key in self.grid_positions:
                    found_position = self.grid_positions[key]
                    break
            
            if found_position:
                logging.info(f"  Grid position: row={found_position.get('row')}, col={found_position.get('column')}")
            else:
                logging.info(f"  Grid position: NOT FOUND (tried keys: {possible_keys})")
        
        logging.info("=== END GRID DEBUG ===")

def compose_from_processor_output(processor_result, output_path):
    """Main composition function called from the processor"""
    try:
        base_dir = processor_result['processed_videos_dir']
        logging.info(f"Using base directory: {base_dir}")
        logging.info("=== Processing Grid Arrangement ===")
        
        # Create composer instance
        composer = VideoComposer(base_dir, processor_result['tracks'], output_path)
        
        # Store validated tracks
        validated_tracks = {}
        
        if 'tracks' in processor_result['processed_files']:
            for instrument, data in processor_result['processed_files']['tracks'].items():
                try:
                    track_idx = data.get('track_idx', 0)
                    
                    # Keep full instrument name, just remove track index if present
                    instrument_parts = instrument.split('_')
                    if instrument_parts[-1].isdigit():
                        instrument_name = '_'.join(instrument_parts[:-1])
                    else:
                        instrument_name = instrument
                    
                    # Construct track path preserving full instrument name
                    track_path = os.path.join(
                        base_dir, 
                        f"track_{track_idx}_{instrument_name}"
                    )
                    
                    logging.info(f"Checking track {track_idx}: {instrument_name}")
                    logging.info(f"Track path: {track_path}")
                    
                    if os.path.exists(track_path):
                        validated_tracks[instrument] = {
                            'base_path': track_path,
                            'instrument_name': instrument_name,
                            'notes': data.get('notes', {})
                        }
                        
                except Exception as e:
                    logging.error(f"Error processing track {instrument}: {str(e)}")
                    continue

        composer.tracks = validated_tracks
        result = composer.create_composition()
        
        return result

    except Exception as e:
        logging.error(f"Error in video composition: {str(e)}")
        raise
